#!/usr/bin/env python3

import argparse
import json
import socket, ssl
from html.parser import HTMLParser
from urllib.parse import urlparse

DEFAULT_SERVER = "project5.3700.network"
DEFAULT_PORT = 443

class LoginHTMLParser(HTMLParser):
    def handle_starttag(self, tag, attrs):
        if tag == "input" and ('name', 'csrfmiddlewaretoken') in attrs:
            for attr in attrs:
                if attr[0] == 'value':
                    self.token = attr[1]
    
    def get_token(self):
        return self.token

class LinkAndFlagHTMLParser(HTMLParser):
    def __init__(self):
        HTMLParser.__init__(self)
        self.links = []
        self.flags = []

    def handle_starttag(self, tag, attrs):
        if tag == "a":
            for name, value in attrs:
                if name == "href" and value != "/" and "logout" not in value:
                    self.links.append(value)

    def handle_data(self, data):
        if "FLAG:" in data:
            self.flags.append(data.split(" ")[-1])
            print(self.flags[-1])

    def reset_links_array(self):
        self.links.clear()

class Crawler:
    def __init__(self, args):
        self.server = args.server
        self.port = args.port
        self.username = args.username
        self.password = args.password
        self.socket = self.get_initialized_socket()
        self.queue, self.seen = [], []
        self.cookies = ""

    def get_initialized_socket(self):
        mysocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        # Wrap socket in TLS so we can use HTTPS
        context = ssl.SSLContext()
        mysocket = context.wrap_socket(mysocket, server_hostname=self.server)
        mysocket.connect((self.server, self.port))
        return mysocket

    def make_request_and_decode_response(self, request):
        #print("Request to %s:%d" % (self.server, self.port))
        #print(request)

        self.socket.send(request.encode('ascii'))

        data = self.socket.recv(10000)
        decoded_data = data.decode('ascii')
        # if 'secret_flag' in decoded_data:
        #     print("Request to %s:%d" % (self.server, self.port))
        #     print(request)
        #     print("Response:\n%s" % decoded_data)

        #print("Response:\n%s" % decoded_data)

        return decoded_data

    def parse_header_into_dict(self, header):
        header = header.splitlines()
        header_d = {}
        header_d['Type'], header_d['Code'] = header[0].split(maxsplit=1)
        header_d['Set-Cookie'] = []
        for line in header[1:]:
            name, value = line.split(': ')
            if name == 'Set-Cookie':
                header_d[name].append(value)
            else:
                header_d[name] = value
        return header_d

    # returns [header_dict, html], where html is a empty string if there is none present
    def get_split_request_response(self, url):
        split_response = self.make_request_and_decode_response(url).split("\r\n\r\n", 1)
        # make sure the array is always of size 2 (to make it easier to work with)
        if len(split_response) == 1:
            split_response.append('')

        return self.parse_header_into_dict(split_response[0]), split_response[1]

    # build get request including cookies and keep-alive connection
    def build_get_request(self, url):
        return ("GET {} HTTP/1.1\r\n"
                "Host: {}\r\n"
                "Connection: keep-alive\r\n"
                "Cookie: {}\r\n\r\n").format(url, self.server, self.cookies)
  
        
    def get_cookies(self, header_dict):
        cookie = ""
        for item in header_dict["Set-Cookie"]:
            if len(cookie) == 0:
                cookie += item.split(";")[0]
            else:
                cookie += "; " + item.split(";")[0]
        return cookie

    # make request, split response and turn header into dict, handle cookies
    # and also handle 302 and 500 error
    def handle_request_and_return_response(self, request):
        header_dict, html = self.get_split_request_response(request)
        if len(header_dict["Set-Cookie"]) > 0:
            self.cookies = self.get_cookies(header_dict)
        
        if header_dict["Connection"] == "close":
            self.socket = self.get_initialized_socket()
            self.login()

        if header_dict["Code"] == "302 Found":
            return self.handle_request_and_return_response(self.build_get_request(header_dict["Location"]))
        elif header_dict["Code"] == "500 Internal Server Error":
            return self.handle_request_and_return_response(request)

        return header_dict, html
        
    def is_forbidden_or_not_found(self, header_dict):
        return header_dict["Code"] in ["403 Forbidden", "404 Not Found"]

    # Login to fakebook
    def login(self):
        # Make initial get request so we can get CSRF cookie
        initial_request = self.build_get_request("/accounts/login/?next=/fakebook/")
        header_dict, html = self.handle_request_and_return_response(initial_request)

        # Parse the login form so we can get csrf middleware token
        login_html_parser = LoginHTMLParser()
        login_html_parser.feed(html)

        # build request body first so we can get length of it
        request_body = "username={}&password={}&csrfmiddlewaretoken={}".format(self.username, self.password, login_html_parser.token)
        # build post request for logging in
        request = ("POST {} HTTP/1.1\r\n"
                   "Host: {}\r\n"
                   "Connection: keep-alive\r\n"
                   "Content-Type: application/x-www-form-urlencoded\r\n"
                   "Content-Length: {}\r\n"
                   "Cookie: {}\r\n\r\n"
                   "{}\r\n\r\n").format("/accounts/login/?next=/fakebook/", self.server, len(request_body), self.cookies, request_body)

        # make login request
        header_dict, html = self.handle_request_and_return_response(request)


    def crawl(self):
        parser = LinkAndFlagHTMLParser()
        while (len(parser.flags) < 5):
            link = self.queue.pop(0)

            self.seen.append(link)
            request = self.build_get_request(link)
            header_dict, html = self.handle_request_and_return_response(request)

            if "Transfer-Encoding" in header_dict:
                print(header_dict["Transfer-Encoding"])
                print(html)

            # Bad url, continue
            if self.is_forbidden_or_not_found(header_dict):
                continue

            parser.feed(html)
            for new_link in parser.links:
                on_specified_server = urlparse(new_link).netloc == ''
                not_in_queue_or_seen = new_link not in self.queue and new_link not in self.seen
                if not_in_queue_or_seen and on_specified_server:
                    self.queue.append(new_link)

            parser.reset_links_array()
            

    
        #print(parser.flags)



    def run(self):
        self.login()
        self.queue = ["/fakebook/"]
        self.crawl()
        

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='crawl Fakebook')
    parser.add_argument('-s', dest="server", type=str, default=DEFAULT_SERVER, help="The server to crawl")
    parser.add_argument('-p', dest="port", type=int, default=DEFAULT_PORT, help="The port to use")
    parser.add_argument('username', type=str, help="The username to use")
    parser.add_argument('password', type=str, help="The password to use")
    args = parser.parse_args()
    sender = Crawler(args)
    sender.run()

#!/usr/bin/env python3

import argparse
import json
import socket, ssl
from html.parser import HTMLParser
from urllib.parse import urlparse

DEFAULT_SERVER = "project5.3700.network"
DEFAULT_PORT = 443

class LoginHTMLParser(HTMLParser):
    def handle_starttag(self, tag, attrs):
        if tag == "input" and ('name', 'csrfmiddlewaretoken') in attrs:
            for attr in attrs:
                if attr[0] == 'value':
                    self.token = attr[1]
    
    def get_token(self):
        return self.token

class LinkAndFlagHTMLParser(HTMLParser):
    def __init__(self):
        HTMLParser.__init__(self)
        self.links = []
        self.flags = []

    def handle_starttag(self, tag, attrs):
        if tag == "a":
            for name, value in attrs:
                if name == "href" and value != "/" and "logout" not in value:
                    self.links.append(value)

    def handle_data(self, data):
        if "FLAG:" in data:
            self.flags.append(data.split(" ")[-1])
            print(self.flags[-1])

    def reset_links_array(self):
        self.links.clear()

class Crawler:
    def __init__(self, args):
        self.server = args.server
        self.port = args.port
        self.username = args.username
        self.password = args.password
        self.socket = self.get_initialized_socket()
        self.queue, self.seen = [], []
        self.cookies = ""

    def get_initialized_socket(self):
        """
        Initialize socket, wrap it in TLS, connect said socket to specified server and port, and return it
        Returns:
            socket - instance of Socket class
        """
        mysocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        # Wrap socket in TLS so we can use HTTPS
        context = ssl.SSLContext()
        mysocket = context.wrap_socket(mysocket, server_hostname=self.server)
        mysocket.connect((self.server, self.port))
        return mysocket

    def make_request_and_decode_response(self, request):
        """
        Make request using given request string and return decoded response
        Args:
            request - str representation of the request
        Returns:
            str - representing decoded response that came from request
        """
        self.socket.send(request.encode('ascii'))

        data = self.socket.recv(10000)
        decoded_data = data.decode('ascii')

        return decoded_data

    def parse_header_into_dict(self, header):
        """
        Parse header into dictionary, where Type and Code are always included
        Args:
            header - str representation of the header
        Returns:
            dictionary - containing all parsable name/value pairs within header
        """
        header_d = {}

        # Initialize array for Set-Cookie if Set-Cookie is present in header
        if 'Set-Cookie' in header:
            header_d['Set-Cookie'] = []

        header = header.splitlines()
        header_d['Type'], header_d['Code'] = header[0].split(maxsplit=1)

        for line in header[1:]:
            name, value = line.split(': ')
            if name == 'Set-Cookie':
                header_d[name].append(value)
            else:
                header_d[name] = value
        return header_d

    def make_request_and_get_split_response(self, request):
        """
        Make request using given request string and return split response
        Args:
            request - str representation of request to be made
        Returns:
            tuple of header str and html str of response (Note html is empty string if none present)
        """
        split_response = self.make_request_and_decode_response(request).split("\r\n\r\n", 1)
        # make sure the array is always of size 2 (to make it easier to work with)
        if len(split_response) == 1:
            split_response.append('')

        return split_response[0], split_response[1]

    def build_get_request(self, url):
        """
        Build get request which includes cookies and keep-alive connection using given url
        Args:
            url - str representation of url
        Returns:
            str - representing full GET request
        """
        return ("GET {} HTTP/1.1\r\n"
                "Host: {}\r\n"
                "Connection: keep-alive\r\n"
                "Cookie: {}\r\n\r\n").format(url, self.server, self.cookies)
  
        
    def get_cookies(self, header_dict):
        cookie = ""
        for item in header_dict["Set-Cookie"]:
            if len(cookie) == 0:
                cookie += item.split(";")[0]
            else:
                cookie += "; " + item.split(";")[0]
        return cookie

    def parse_chunked_response(self, html):
        """
        Parse chunked html response and return unchunked html
        Args:
            html - str representation of chunked html
        Returns:
            str - representing unchunked html
        """
        unchunked_html = ""
        chunked_html_split = html.split("\r\n")
        curr_idx = 0
        curr_len = int(chunked_html_split[curr_idx], 16)
        while (curr_len != 0):
            unchunked_html += chunked_html_split[curr_idx + 1][0:curr_len]
            curr_idx += 2
            curr_len = int(chunked_html_split[curr_idx], 16)

        return unchunked_html

    def handle_request_and_return_response(self, request):
        """
        Make request, process response, process cookies, and handle various scenarios before returning response
        These scenarios include:
            reinitializing connection if response indicates that connection will be closed
            converting chunked response into unchunked if necessary
            handling 302 and 500 response codes
        Args:
            request - str representation of request to be made
        Returns:
            tuple of header dictionary and html string of response
        """
        header, html = self.make_request_and_get_split_response(request)
        header_dict = self.parse_header_into_dict(header)
        
        if "Set-Cookie" in header_dict:
            self.cookies = self.get_cookies(header_dict)
        
        if header_dict["Connection"] == "close":
            self.socket = self.get_initialized_socket()
            self.login()

        if "Transfer-Encoding" in header_dict and header_dict["Transfer-Encoding"] == 'chunked':
            html = self.parse_chunked_response(html)

        if header_dict["Code"] == "302 Found":
            return self.handle_request_and_return_response(self.build_get_request(header_dict["Location"]))
        elif header_dict["Code"] == "500 Internal Server Error":
            return self.handle_request_and_return_response(request)

        return header_dict, html
        
    def is_forbidden_or_not_found(self, header_dict):
        return header_dict["Code"] in ["403 Forbidden", "404 Not Found"]

    # Login to fakebook
    def login(self):
        # Make initial get request so we can get CSRF cookie
        initial_request = self.build_get_request("/accounts/login/?next=/fakebook/")
        header_dict, html = self.handle_request_and_return_response(initial_request)

        # Parse the login form so we can get csrf middleware token
        login_html_parser = LoginHTMLParser()
        login_html_parser.feed(html)

        # build request body first so we can get length of it
        request_body = "username={}&password={}&csrfmiddlewaretoken={}".format(self.username, self.password, login_html_parser.token)
        # build post request for logging in
        request = ("POST {} HTTP/1.1\r\n"
                   "Host: {}\r\n"
                   "Connection: keep-alive\r\n"
                   "Content-Type: application/x-www-form-urlencoded\r\n"
                   "Content-Length: {}\r\n"
                   "Cookie: {}\r\n\r\n"
                   "{}\r\n\r\n").format("/accounts/login/?next=/fakebook/", self.server, len(request_body), self.cookies, request_body)

        # make login request
        header_dict, html = self.handle_request_and_return_response(request)


    def crawl(self):
        parser = LinkAndFlagHTMLParser()
        while (len(parser.flags) < 5):
            link = self.queue.pop(0)

            self.seen.append(link)
            request = self.build_get_request(link)
            header_dict, html = self.handle_request_and_return_response(request)

            # Bad url, continue
            if self.is_forbidden_or_not_found(header_dict):
                continue

            parser.feed(html)
            for new_link in parser.links:
                on_specified_server = urlparse(new_link).netloc == ''
                not_in_queue_or_seen = new_link not in self.queue and new_link not in self.seen
                if not_in_queue_or_seen and on_specified_server:
                    self.queue.append(new_link)

            parser.reset_links_array()
            

    def run(self):
        self.login()
        self.queue = ["/fakebook/"]
        self.crawl()
        

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='crawl Fakebook')
    parser.add_argument('-s', dest="server", type=str, default=DEFAULT_SERVER, help="The server to crawl")
    parser.add_argument('-p', dest="port", type=int, default=DEFAULT_PORT, help="The port to use")
    parser.add_argument('username', type=str, help="The username to use")
    parser.add_argument('password', type=str, help="The password to use")
    args = parser.parse_args()
    sender = Crawler(args)
    sender.run()

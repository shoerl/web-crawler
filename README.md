# CS3700 Project 5: Web Crawler
Group Members: Sean Hoerl & Jess Van de Ven

## High-Level Approach
At a high level, our crawler logs in to fakebook, and then begins crawling until we have seen all 5 secret flags. We use HTTP 1.1 for all of the requests. For logging in, we send an initial GET request to the login page of fakebook and obtain all of the relevant info there that we need for logging in. This relevant info includes the csrftoken cookie and the csrfmiddleware token which are both needed in order to successfully login. We then build our POST request for the login page using the cookies and token that we just grabbed in the GET request, and the username and password which were provided in the command arguments. If this login POST request runs successfully and responds with the proper code, we will begin to crawl.

For crawling, we start at the home page of fakebook, and grab all available links there and then add them to our queue. For each link in our queue, we visit the corresponding page, check to see if a secret flag is present, grab all unseen links present on the page (that point to the target domain) and add them to our queue. For each page, we ignore the links that point to the home page and the logout page, as the home page is the first thing we crawl, and we do not want to logout at any point. We do this until we have gathered all 5 secret flags.

## Challenges
There were a few things that came up during development of this project that we had difficulty with. 

The first was in regards to how we dealt with parsing the header of any HTTP response. We initially just parsed it straight into a dictionary, splitting each line and adding the corresponding name/value pair to the dictionary. But this was problematic as when submitting a successful login POST request, we get two lines of Set-Cookie in the header of the HTTP response. In our first attempt of dealing with this, we tried to abstract out the behavior and initially have the value as a string (so for the first instance of seeing it), and then if we see a second instance of that name, make an array containing the old value and the new value, and then if we see any more instances after that, append it to said array. Doing it this way, we had to use isinstance, so we didn't actually have to hardcode Set-Cookie into the function. While this way of doing it allowed us to abstract out more, we then had to do type checking in our method which built the cookie string for our HTTP requests (as it could be a string or an array). We thought that the type checking (isinstance) there was ugly and unintuitive, so we decided to just always have Set-Cookie be an array within our dictionary, which would have 1 or more items. We also check to see if the Set-Cookie string is in the header before initializing said array. For any name, value pair we simply append to the array if the name is Set-Cookie. While this is a little bit less abstract, we figured that since Set-Cookie was the only item we would ever see twice or more times in the header it was fine, and it also was a bit simpler. Since none of the possible solutions were great, we simply opted for the simplest and most understandable one out of them.

The second was figuring out how to properly deal with chunked html messages when they were never actually returned from fakebook. To do accomplish this, we used online examples of chunked responses while developing the function. Then for testing the function, we used hardcoded in one of those examples to ensure the function ran properly.

The third was figuring out where to deal with certain status codes. We have a function that handles certain scenarios that can take place after submitting a request, and within this function we deal with 302 Found code and 500 Internal Server Error code. For the 302 Found code, you just have to make another request to the link that the response is pointing you to, so because of that you don't care about the actual html response of the previous request, thus it made sense to deal with that within said function. For the 500 Internal Server Error code, you simply want to contiously retry the request until it completes successfully, so it made sense to deal with that within said function as the only response you care about is the one that finally works. We got a bit confused about where to deal with the 403 Forbidden and 404 Not Found codes, as for those codes you simply want to ignore the url and continue crawling. Since our function that handles requests returns the header_dict and html that comes from the response, we didn't see a straightforward way to indicate to the caller of the method that you should ignore this response. We could have returned a empty string for the html to indicate this, but it is possible that we recieve an empty string as the html for other responses, so we didn't want to give any false positives or conflate them. Thus we found that the best solution was to just return the header_dict and html as we normally do, and leave it up to the caller of the method to check for those codes and ignore those urls.

## Properties/Features

We believe we did a good job putting distinctive behavior and logic into functions, in an effort to make the code more readable and easier to work with. We also think converting the header into a dictionary was a good decision, as it made obtaining information from the HTTP header much easier. Wrapping all of the things which you have to handle every time you get a request response into a function was also a good choice, as it helped reduce code duplication. These things include checking if there were any Set-Cookies present and updating our cookies if so, reinitializing the connection if the response indicates the connection is about to be closed, unchunking a chunked HTML response, and dealing with 302 Found and 500 Internal Server Error codes. Using the HTMLParser was also a good choice, as it allowed us to wrap the corresponding logic for parsing certain elements from the HTML into classes, which we were able to use in our crawler. We also raise a RuntimeError if there is an issue logging in (we don't get a sessionid cookie), which we believe was a good choice as it fails fast and won't try to start crawling if there is an issue early on.

For the actual crawling, we maintained two lists, queue and seen. We used queue as a queue, popping the first item off each iteration. The queue represented the queue of links we wanted to visit. We treated seen as a regular list which contained all of the links/urls which we visited. When determining whether to add a new url to our queue, we first check if the netlocation of it is empty, which is essentially checking if the target domain points to our specified server. We thought this was a simple and intuitive way to deal with only crawling the target domain. We also check to make sure that the link we want to add is not in our queue already and is not in our seen list, which ensures we don't visit any links twice. 

## Testing

We developed our crawler iteratively, first coming up with a working solution using HTTP 1.0, and then tackling HTTP 1.1. Before coming up with any of the logic/behavior for crawling, we first dealt with logging in, which meant dealing with basic get/post requests. So when figuring out the login logic, there were only a small number of requests and responses, thus we were able to print out everything and see exactly what was coming back. This made it easier to debug and figure out exactly what was going on. Additionally, the HTTP Made Really Easy tutorial helped us get started with making basic HTTP responses. The developer tools of the chrome browser were also very useful in figuring out exactly what needed to be done in regards to the POST requests for logging in. So a lot of the initial testing was done by just trial and error and analyzing the output from our responses. For other functions that weren't directly dealing with responses (turning header into dictionary and dealing with chunked HTML), we were able to pull those out and test them with mocked input. We then were able to analyze the output for those functions to ensure they worked exactly as we wanted. 

For testing the logic of our crawler, we had to take advantage of print statements and logs to ensure the behavior of our crawler matched exactly what we were expecting. We initially had an error in our logic for whether or not to append an item to our queue, and by using these print statements/logs we were able to catch that quite early and figure that out.